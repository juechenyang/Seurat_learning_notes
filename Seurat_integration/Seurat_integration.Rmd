---
title: "Integration pipeline"
author: "Juechen Yang"
date: "4/13/2022"
output: rmarkdown::github_document
---

# load libraries
```{r warning=FALSE, message=FALSE}
library(Seurat)
library(SeuratData)
library(patchwork)
#InstallData("ifnb")
LoadData("ifnb")
```

# learn FindVariableFeatures

## create object data for convenience
```{r}
ifnb.list <- SplitObject(ifnb, split.by = "stim")
ifnb.list <- lapply(X = ifnb.list, FUN = function(x) {
  x <- NormalizeData(x)
  x <- FindVariableFeatures(x, selection.method = "vst", nfeatures = 2000)
})
object = ifnb.list[[1]]
#normalization
object = NormalizeData(object = object, verbose = F)
```

## get assay data for object

Because FindVariableFeatures needs the count matrix for input, so it is required to fetch the assay data from the Seurat object for downstream calculation

```{r}
object = GetAssayData(object)
#initializa verbose
verbose = FALSE
clip.max = "auto"
if (clip.max == "auto") {
      clip.max <- sqrt(x = ncol(x = object))
}
```

## computes the mean and variance for each row(gene)

```{r warning=FALSE, message=FALSE}
SparseRowVar2 <- function(mat, mu, display_progress) {
    .Call('_Seurat_SparseRowVar2', PACKAGE = 'Seurat', mat, mu, display_progress)
}
hvf.info <- data.frame(mean = rowMeans(x = object))
hvf.info$variance <- SparseRowVar2(mat = object, mu = hvf.info$mean, 
                      display_progress = verbose)
hvf.info[1:10,]
```

## assign 0 to some metrics (I don't know why doing this)

```{r}
hvf.info$variance.expected <- 0
hvf.info$variance.standardized <- 0
not.const <- hvf.info$variance > 0
```

## produce a fit using loess

This is try to regress and smooth the curve of log10(mean) and log10(var). Loess Regression is the most common method used to smooth a volatile time series. It is a non-parametric methods where least squares regression is performed in localized subsets, which makes it a suitable candidate for smoothing any numerical vector.

```{r warning=FALSE, message=FALSE}
loess.span = 0.3
fit <- loess(formula = log10(x = variance) ~ log10(x = mean), 
      data = hvf.info[not.const, ], span = loess.span)
```

## get the expected variance for gene that has larger than 0 variance

```{r warning=FALSE, message=FALSE}
hvf.info$variance.expected[not.const] <- 10^fit$fitted
```

## calculate variance of zscore which is captured by mean and expected variance

```{r}
SparseRowVarStd <- function(mat, mu, sd, vmax, display_progress) {
    .Call('_Seurat_SparseRowVarStd', PACKAGE = 'Seurat', mat, mu, sd, vmax, display_progress)
}
hvf.info$variance.standardized <- SparseRowVarStd(mat = object, 
      mu = hvf.info$mean, sd = sqrt(hvf.info$variance.expected), 
      vmax = clip.max, display_progress = verbose)
```

## we selected top n genes that have highest standardized variance
```{r}
hvf.info = hvf.info[order(hvf.info$variance.standardized, decreasing = T),]
top_n = 2000
top.genes = rownames(hvf.info)[1:top_n]
top.genes[1:20]
```

# learn SelectIntegrationFeatures

```{r}
features <- SelectIntegrationFeatures(object.list = ifnb.list)
```

## initializa parameters and preprocess
```{r warning=FALSE}

nfeatures = 2000
assay = NULL
verbose = TRUE
fvf.nfeatures = 2000
object.list = ifnb.list
```

## assign assays to each obj in the list

```{r}
if (!is.null(x = assay)) {
    if (length(x = assay) != length(x = object.list)) {
      stop("If specifying the assay, please specify one assay per object in the object.list")
    }
    for (ii in length(x = object.list)) {
      DefaultAssay(object = object.list[[ii]]) <- assay[ii]
    }
}else {
  assay <- sapply(X = object.list, FUN = DefaultAssay)
}
```

## check if variable features exist

```{r}
for (ii in 1:length(x = object.list)) {
  if (length(x = VariableFeatures(object = object.list[[ii]])) == 
    0) {
    if (verbose) {
      message(paste0("No variable features found for object", 
        ii, " in the object.list. Running FindVariableFeatures ..."))
    }
    object.list[[ii]] <- FindVariableFeatures(object = object.list[[ii]], 
      nfeatures = fvf.nfeatures, verbose = verbose, 
      ...)
  }
}
```

## fetch highly variable feature names from all seurat obj in the list

```{r}
var.features <- unname(obj = unlist(x = lapply(X = 1:length(x = object.list), 
    FUN = function(x) VariableFeatures(object = object.list[[x]], 
      assay = assay[x]))))
```

## find shared highly variable genes

```{r}
var.features <- sort(x = table(var.features), decreasing = TRUE)
#make sure highly variable genes should present in all seurat obj 
#(even may not highly variable in every)
for (i in 1:length(x = object.list)) {
  var.features <- var.features[names(x = var.features) %in% 
    rownames(x = object.list[[i]][[assay[i]]])]
}
```

## select features that have highly variable freq larger than tie value

```{r}
tie.val <- var.features[min(nfeatures, length(x = var.features))]
features <- names(x = var.features[which(x = var.features > 
    tie.val)])
length(features)
features[1:20]
```

## fetch variable features for each dataset

```{r}
vf.list <- lapply(X = object.list, FUN = VariableFeatures)
vf.list[[1]][1:10]
```

## re-rank features based on their median rank for all vairable gene set from each dataset 

```{r}
#features are genes that shared by more than tie.val datasets
if (length(x = features) > 0) {
  feature.ranks <- sapply(X = features, FUN = function(x) {
    ranks <- sapply(X = vf.list, FUN = function(vf) {
      if (x %in% vf) {
        return(which(x = x == vf))
      }
      return(NULL)
    })
    median(x = unlist(x = ranks))
  })
  features <- names(x = sort(x = feature.ranks))
}
```

## grab the features that equal to tie.val

```{r}
features.tie <- var.features[which(x = var.features == tie.val)]
```

## do the same re-rank for tie features

```{r}
tie.ranks <- sapply(X = names(x = features.tie), FUN = function(x) {
  ranks <- sapply(X = vf.list, FUN = function(vf) {
    if (x %in% vf) {
      return(which(x = x == vf))
    }
    return(NULL)
  })
  median(x = unlist(x = ranks))
})
```

## return a vector with features in the front and tie.features in the back

```{r}
features <- c(features, names(x = head(x = sort(x = tie.ranks), 
    nfeatures - length(x = features))))
```

# learn FindIntegrationAnchors

```{r include=FALSE}
#immune.anchors <- FindIntegrationAnchors(object.list = ifnb.list, anchor.features = features)
```

## initailiza parameters

```{r}
object.list = ifnb.list
assay = NULL
reference = NULL 
anchor.features = features
scale = TRUE
normalization.method = c("LogNormalize", "SCT")
sct.clip.range = NULL
reduction = c("cca", "rpca", "rlsi")
l2.norm = TRUE
dims = 1:30
k.anchor = 5 
k.filter = 200
k.score = 30
max.features = 200
nn.method = "annoy" 
n.trees = 50
eps = 0
verbose = FALSE
```

## finalized methods
```{r}
normalization.method <- normalization.method[1]
reduction <- reduction[1]
```

## define a future function
```{r}
library("future.apply")
my.lapply <- ifelse(test = verbose && nbrOfWorkers() == 
    1, yes = pblapply, no = future_lapply)
```

## get number of cells for each dataset

```{r}
object.ncells <- sapply(X = object.list, FUN = function(x) dim(x = x)[2])
```

## get default assay for each dataset
```{r}
assay <- sapply(X = object.list, FUN = DefaultAssay)
```

## check duplicate cell names
```{r}
# initialize slot tools for Integration
object.list <- lapply(X = object.list, FUN = function(obj) {
  slot(object = obj, name = "tools")$Integration <- NULL
  return(obj)
})
# check duplicate cell names
CheckDuplicateCellNames <- function(object.list, verbose = TRUE, stop = FALSE) {
  cell.names <- unlist(x = lapply(X = object.list, FUN = colnames))
  if (any(duplicated(x = cell.names))) {
    if (stop) {
      stop("Duplicate cell names present across objects provided.")
    }
    if (verbose) {
      warning("Some cell names are duplicated across objects provided. Renaming to enforce unique cell names.")
    }
    object.list <- lapply(
      X = 1:length(x = object.list),
      FUN = function(x) {
        return(RenameCells(
          object = object.list[[x]],
          new.names = paste0(Cells(x = object.list[[x]]), "_", x)
        ))
      }
    )
  }
  return(object.list)
}
object.list <- CheckDuplicateCellNames(object.list = object.list)
```

## scale data for only those selected features for integration
```{r}
slot <- "data"
object.list <- my.lapply(X = object.list, FUN = function(object){
  ScaleData(object = object, features = anchor.features, 
    verbose = FALSE)
})
nn.reduction <- reduction
internal.neighbors <- list()
```

## build combinations
```{r}
combinations <- expand.grid(1:length(x = object.list), 1:length(x = object.list))
combinations <- combinations[combinations$Var1 < combinations$Var2, 
    , drop = FALSE]
```

## get cumulative number of cells without last dataset
```{r}
objects.ncell <- sapply(X = object.list, FUN = ncol)
offsets <- as.vector(x = cumsum(x = c(0, objects.ncell)))[1:length(x = object.list)]
```

## create diet seurat obj
```{r}

```






